{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsTljco25Nyr"
   },
   "source": [
    "# Chapter 3 Tutorial #2: Approaches to Prompt Engineering\n",
    "\n",
    "In Chapter 3 we discussed various methods of template engineering with the goal of improving the analytic capabilities of prompt-tuned LLMs. As part of this discussion, we demonstrated online web applications that could be harnessed to do prompt-based inference and information retrieval, demonstrating the sensitivity of results to choices in template architecture and the fine details of prompt composition.\n",
    "\n",
    "While these web applications are valuable for exploring certain features of masked language prediction, they are limited in their capability for full prompt-based fine-tuning solutions. As such in this tutorial, we will expand on these exercises by exploring few- and many-shot prompt-tuning, discussing results for variable prompt template designs, with a goal towards grasping the critical importance of prompt template optimization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gSSZFq6OvEG"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJu37RUGpwuP",
    "outputId": "1c9cb76a-b118-4ec5-81fe-f3b0085b6709"
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.19.0\n",
    "%pip install datasets\n",
    "%pip install openprompt\n",
    "%pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNaSQwd2pwn_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from datasets import load_dataset\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, MixedTemplate, SoftTemplate\n",
    "from openprompt.prompts.prefix_tuning_template import PrefixTuningTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "set_seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVzIg71SOzc0"
   },
   "source": [
    "## Data loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5CoaGh_XpRV"
   },
   "outputs": [],
   "source": [
    "## Load the SuperBLUE BoolQ dataset. THis consists of triplets of an informational paragraph,\n",
    "## a yes or no question related to the content of the paragraph, and the correct response.\n",
    "## Save these into test, train, and validation splits.\n",
    "\n",
    "def read_glue_boolq_dataset(tokenizer=None, max_len=512):\n",
    "    ogdataset = load_dataset('super_glue','boolq')\n",
    "\n",
    "    ogdataset['train'] = ogdataset['train'].shuffle(seed=1989).select(range(5000))\n",
    "    ogdataset['test'] = ogdataset['validation'].shuffle(seed=1989).select(range(1800,3270))\n",
    "    ogdataset['validation'] = ogdataset['validation'].shuffle(seed=1989).select(range(1800))\n",
    "\n",
    "    # create a dataset of opemprompt InputExamples from the training data\n",
    "    from openprompt.data_utils import InputExample\n",
    "    dataset = {}\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in ('test','validation'): ipos, ineg = 0, 0\n",
    "        dataset[split] = []\n",
    "        for data in ogdataset[split]:\n",
    "            ## Throw away samples that are too long.\n",
    "            if tokenizer != None:\n",
    "                testlen = data['passage']+' '+data['question']\n",
    "                if len(tokenizer.tokenize(testlen)) > max_len:\n",
    "                    continue\n",
    "            ## Getting 50/50 for validation\n",
    "            if split in ('test','validation'):\n",
    "                if data['label'] in (1,'1'):\n",
    "                     if ipos >= 500: continue\n",
    "                     ipos += 1\n",
    "                if data['label'] in (0,'0'):\n",
    "                     if ineg >= 500: continue\n",
    "                     ineg += 1\n",
    "            ## Create input examples\n",
    "            input_example = InputExample(text_a = data['passage'],\n",
    "                                        text_b = data['question'],\n",
    "                                        label=data['label'],\n",
    "                                      # idx=data['idx'],\n",
    "                                        guid = \"%s-%s\" % (split, data['idx']))\n",
    "            dataset[split].append(input_example)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD3rbnluPBsI"
   },
   "source": [
    "## Prompt-tuning functions and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnXrMTxg-ZJv"
   },
   "outputs": [],
   "source": [
    "## SOFT PROMPT TUNING\n",
    "\n",
    "class SoftPromptTuningArgs:\n",
    "    seed = 144\n",
    "    plm_eval_mode = True\n",
    "    tune_plm = False\n",
    "    max_steps = 20000\n",
    "    prompt_lr = 0.3\n",
    "    warmup_step_prompt = 500\n",
    "    soft_token_num = 20\n",
    "\n",
    "\n",
    "class FixedPromptTuningArgs:\n",
    "    seed = 144\n",
    "    plm_eval_mode = False\n",
    "    tune_plm = True\n",
    "    max_steps = 100 #20000\n",
    "    prompt_lr = 0.3\n",
    "    warmup_step_prompt = 500\n",
    "    soft_token_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6teTpZggi8PZ"
   },
   "outputs": [],
   "source": [
    "## Create a two-parameter verbalizer mapped to \"yes\" and \"no\"\n",
    "def set_verbalizer_model(mytemplate, tokenizer, plm, use_cuda):\n",
    "    myverbalizer = ManualVerbalizer(tokenizer, label_words=[['no','No'],['yes','Yes']], num_classes = 2)\n",
    "    prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer,\n",
    "                                           freeze_plm = False)\n",
    "    if use_cuda:\n",
    "        prompt_model = prompt_model.cuda()\n",
    "    return prompt_model\n",
    "\n",
    "\n",
    "## OpenPrompt dataloader routine\n",
    "def create_dataloader(dataset, mytemplate, tokenizer, WrapperClass, max_seq_l, batchsize_t,\n",
    "                      shuffle, seed):\n",
    "\n",
    "    the_dataloader = PromptDataLoader(dataset=dataset, template=mytemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "        batch_size=batchsize_t,shuffle=shuffle, teacher_forcing=False,\n",
    "        predict_eos_token=False, truncate_method=\"tail\",seed=seed)\n",
    "    return the_dataloader\n",
    "\n",
    "\n",
    "## Implement standard accuracy metric for assessing prediction quality.\n",
    "def evaluate(prompt_model, dataloader, use_cuda, desc):\n",
    "    prompt_model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc\n",
    "\n",
    "\n",
    "## Configuring LLM and prompt for fine-tuning.\n",
    "def set_plm_prompt_optimizers(prompt_model, tune_plm, tune_prompt, tot_step,\n",
    "                              learn_rate,args):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer1 = None\n",
    "    scheduler1 = None\n",
    "    optimizer2 = None\n",
    "    scheduler2 = None\n",
    "\n",
    "    if tune_plm:\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters1 = [\n",
    "            {'params': [p for n, p in prompt_model.plm.named_parameters() if (not any(nd in n for nd in no_decay))], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer1 = AdamW(optimizer_grouped_parameters1, lr=learn_rate, ## lr defaults to 3e-5\n",
    "                           no_deprecation_warning=True)\n",
    "        scheduler1 = get_linear_schedule_with_warmup(\n",
    "            optimizer1,\n",
    "            num_warmup_steps=0, num_training_steps=tot_step)\n",
    "\n",
    "    if tune_prompt:\n",
    "        optimizer_grouped_parameters2 = [{'params': [p for name, p in prompt_model.template.named_parameters() if 'raw_embedding' not in name]}]\n",
    "        optimizer2 = AdamW(optimizer_grouped_parameters2, lr=0.4, no_deprecation_warning=True) ## lr defaults to 0.4\n",
    "        scheduler2 = get_linear_schedule_with_warmup(\n",
    "                        optimizer2,\n",
    "                        num_warmup_steps=args.warmup_step_prompt, num_training_steps=tot_step)\n",
    "\n",
    "    return [loss_func, optimizer1, scheduler1, optimizer2, scheduler2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLjMMEFtY5Lh"
   },
   "outputs": [],
   "source": [
    "def full_program(task, llm, prompt_type, dataset_name, template, training_sizes,\n",
    "                 num_epoch, learn_rate=3e-5, allow_plm_to_vary = True, allow_prompt_to_vary = True,\n",
    "                 starting_mod = None, use_cuda = True):\n",
    "\n",
    "    ## Collect the relevant arguments\n",
    "    if prompt_type == 'fixed':\n",
    "        args = FixedPromptTuningArgs()\n",
    "    elif prompt_type in ('soft','mixed','prefix'):\n",
    "        args = SoftPromptTuningArgs()\n",
    "    else:\n",
    "        print('Unrecognized prompt_type')\n",
    "        return 0\n",
    "\n",
    "    ## Set parameters based on whether the LLM is being tuned or not ##\n",
    "    max_seq_l = 512\n",
    "    tot_step = args.max_steps\n",
    "    if args.tune_plm:\n",
    "        batchsize_t = 4\n",
    "        batchsize_e = 4\n",
    "        gradient_accumulation_steps = 8\n",
    "        model_parallelize = True\n",
    "    else:\n",
    "        batchsize_t = 8\n",
    "        batchsize_e = 4\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "\n",
    "    ## Read in the model ##\n",
    "    if llm == 't5':\n",
    "        plm, tokenizer, model_config, WrapperClass = load_plm('t5', 't5-base')\n",
    "    else:\n",
    "        print('Unrecognized llm type')\n",
    "        return 0\n",
    "\n",
    "    ## Read in the data ##\n",
    "    dataset = read_glue_boolq_dataset(tokenizer=tokenizer,max_len=399)\n",
    "\n",
    "    ## Create the OpenPrompt objects ##\n",
    "    if prompt_type == 'fixed':\n",
    "        mytemplate = ManualTemplate(tokenizer=tokenizer, text=template)\n",
    "    elif prompt_type == 'soft':\n",
    "        mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=args.soft_token_num, text=template)\n",
    "    elif prompt_type == 'mixed':\n",
    "        mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text=template)\n",
    "    elif prompt_type == 'prefix':\n",
    "        mytemplate = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text=template,\n",
    "                                          using_decoder_past_key_values=False).cuda(device='cuda:0')\n",
    "\n",
    "    ## Print out sample wrapped examples to make sure format is correct ##\n",
    "    wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "    print('One wrapped example:', wrapped_example,'\\n')\n",
    "    wrapped_example = mytemplate.wrap_one_example(dataset['validation'][200])\n",
    "    print('One wrapped example:', wrapped_example,'\\n')\n",
    "\n",
    "    # Separate positives and negatives in the train sample and make dataloaders ##\n",
    "    ts_datasets = []\n",
    "    dataset['pos_train'] = [dataset['train'][i] for i in range(len(dataset['train'])) if dataset['train'][i].label == 1]\n",
    "    dataset['neg_train'] = [dataset['train'][i] for i in range(len(dataset['train'])) if dataset['train'][i].label == 0]\n",
    "\n",
    "    ## Create datasets for each training size, with even numbers of positive and negative ##\n",
    "    maxts = max(training_sizes)\n",
    "    dataset_comb = []\n",
    "    for a,b in zip(dataset['pos_train'][:maxts], dataset['neg_train'][:maxts]):\n",
    "        dataset_comb.append(a)\n",
    "        dataset_comb.append(b)\n",
    "    for ts in training_sizes:\n",
    "        if ts==0:\n",
    "            ts_datasets.append('nan')\n",
    "            continue\n",
    "        ts_datasets.append(dataset_comb[:2*ts])\n",
    "\n",
    "\n",
    "    ## Create validation and test dataloader ##\n",
    "    validation_dataloader = create_dataloader(dataset['validation'], mytemplate,\n",
    "                                              tokenizer, WrapperClass, max_seq_l,\n",
    "                                              batchsize_t,shuffle=False,seed=args.seed)\n",
    "    test_dataloader = create_dataloader(dataset['test'], mytemplate,\n",
    "                                        tokenizer, WrapperClass, max_seq_l,\n",
    "                                        batchsize_t,shuffle=False,seed=args.seed)\n",
    "\n",
    "    ## Training loop for specified train set sizes ##\n",
    "    val_accs = []\n",
    "    print(training_sizes)\n",
    "    for ii, ts in enumerate(training_sizes):\n",
    "\n",
    "        ## Create prompt model with template and model. If starting from an\n",
    "        ## existing model, read that in instead.\n",
    "        if starting_mod == None:\n",
    "            prompt_model = set_verbalizer_model(mytemplate,\n",
    "                                                tokenizer,\n",
    "                                                copy.deepcopy(plm),\n",
    "                                                use_cuda)\n",
    "        else:\n",
    "            print('USING MODEL THAT WAS PASSED IN')\n",
    "            prompt_model = copy.deepcopy(starting_mod)\n",
    "\n",
    "        ## Save number of model variables\n",
    "        prompt_params = sum(p.numel() for p in prompt_model.parameters())\n",
    "        llm_params = sum(p.numel() for p in plm.parameters())\n",
    "        print(f\"Number of parameters in prompt model: {prompt_params}\")\n",
    "        print(f\"Number of parameters in LLM: {llm_params}\")\n",
    "\n",
    "        ## Zero-shot evaluation against the validation set if 0 in ts.\n",
    "        if ts == 0:\n",
    "            val_accs.append(evaluate(prompt_model, validation_dataloader, use_cuda, desc=\"Valid\"))\n",
    "            print('Zero-shot val_acc =',val_accs)\n",
    "            continue\n",
    "\n",
    "        prompt_model.train()\n",
    "\n",
    "        ## Create training data loader\n",
    "        ts_dataloader = create_dataloader(ts_datasets[ii],mytemplate,\n",
    "                                          tokenizer, WrapperClass,\n",
    "                                          max_seq_l, batchsize_t,\n",
    "                                          shuffle=True,seed=args.seed)\n",
    "\n",
    "        tot_step = len(ts_dataloader)*num_epoch\n",
    "\n",
    "        ## Set optimization parameters for LLM and Prompt.\n",
    "        if prompt_type == 'fixed':\n",
    "            opt_params = set_plm_prompt_optimizers(prompt_model=prompt_model,\n",
    "                                                  tune_plm=True,\n",
    "                                                  tune_prompt=False,\n",
    "                                                  tot_step=tot_step,\n",
    "                                                  learn_rate=learn_rate,\n",
    "                                                  args=args)\n",
    "        elif prompt_type in ('soft','mixed','prefix'):\n",
    "            opt_params = set_plm_prompt_optimizers(prompt_model=prompt_model,\n",
    "                                                  tune_plm=allow_plm_to_vary,\n",
    "                                                  tune_prompt=allow_prompt_to_vary,\n",
    "                                                  tot_step=tot_step,\n",
    "                                                  learn_rate=learn_rate,\n",
    "                                                  args=args)\n",
    "        loss_func, optimizer1, scheduler1, optimizer2, scheduler2 = opt_params\n",
    "\n",
    "        actual_step = 0\n",
    "        tot_train_time = 0\n",
    "        best_val_acc = 0\n",
    "\n",
    "        ## Run the training and validation loop\n",
    "        for epoch in range(num_epoch):\n",
    "            tot_loss = 0\n",
    "            for step, inputs in enumerate(ts_dataloader):\n",
    "                prompt_model.train()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                tot_train_time -= time.time()\n",
    "\n",
    "                logits = prompt_model(inputs)\n",
    "                labels = inputs['label']\n",
    "                loss = loss_func(logits, labels)\n",
    "                loss.backward()\n",
    "                tot_loss += loss.item()\n",
    "                actual_step += 1\n",
    "                torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
    "\n",
    "                if optimizer1 is not None:\n",
    "                    optimizer1.step()\n",
    "                    scheduler1.step()\n",
    "                    optimizer1.zero_grad()\n",
    "                if optimizer2 is not None:\n",
    "                    optimizer2.step()\n",
    "                    scheduler2.step()\n",
    "                    optimizer2.zero_grad()\n",
    "\n",
    "                tot_train_time += time.time()\n",
    "\n",
    "            ## Print out epoch-to-epoch changes if tuning prompt\n",
    "            if prompt_type == 'soft' and allow_prompt_to_vary and epoch+1 in (1,10,20,30,40,50,60):\n",
    "                val_acc = evaluate(prompt_model, validation_dataloader, use_cuda, desc=\"Valid\")\n",
    "                if val_acc >= best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                val_accs.append(val_acc)\n",
    "                print('In epoch ',epoch+1,' after step', actual_step,', best val acc = %.4f'%best_val_acc)\n",
    "\n",
    "        ## Predict with current model against train and validation sets for logging.\n",
    "        train_acc = evaluate(prompt_model, ts_dataloader, use_cuda, desc=\"Train\")\n",
    "        val_acc = evaluate(prompt_model, validation_dataloader, use_cuda, desc=\"Valid\")\n",
    "        val_accs.append(val_acc)\n",
    "        print('Train acc = ',train_acc,'  |  Val acc = ',val_acc)\n",
    "\n",
    "    ## With training loop done, run against indepedent test set for final check.\n",
    "    print('val_accs =', val_accs)\n",
    "    test_acc = evaluate(prompt_model, test_dataloader, use_cuda, desc=\"Valid\")\n",
    "    print('Test accuracy = ',test_acc)\n",
    "    return prompt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRcGMwhnRaOW"
   },
   "source": [
    "## Test #1\n",
    "Fine-tune a t5-base model with three different template shapes of increasing complexity and compare the zero-shot and few-shot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsBiPc-kdGl3"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer.\n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "prompt_type = 'fixed' ## Use a fixed, or \"manual\", template style.\n",
    "\n",
    "llm = 't5'\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 5\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-jXg2RRRnTi"
   },
   "outputs": [],
   "source": [
    "## Simplest template\n",
    "templ1 = '''{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ1,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wG3sbXd7Rn0O"
   },
   "outputs": [],
   "source": [
    "## Simplest plus punctuation\n",
    "templ2 = '''{\"placeholder\":\"text_a\", \"shortenable\":False} . {\"placeholder\":\"text_b\"} ? {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ2,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nqi-Qh2RkCs"
   },
   "outputs": [],
   "source": [
    "## \"Suitable\" template\n",
    "templ3 = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} premise: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer was {\"mask\"} .'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ3,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2Z7Ya9pN2Wt"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 1\n",
    "fig1_x = (0, 16, 32, 64, 128, 256, 512)\n",
    "\n",
    "fig1_templ1 = [0.501, 0.511, 0.513, 0.518, 0.526, 0.578, 0.546]\n",
    "fig1_templ1test = 0.526\n",
    "\n",
    "fig1_templ2 = [0.526, 0.508, 0.482, 0.516, 0.598, 0.628, 0.638]\n",
    "fig1_templ2test = 0.609\n",
    "\n",
    "fig1_templ3 = [0.564, 0.489, 0.503, 0.535, 0.638, 0.686, 0.640]\n",
    "fig1_templ3test = 0.649"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-mUq7wR0c3"
   },
   "source": [
    "## Test #2\n",
    "Fine-tune a t5-base model with 10 different templates, representing variants of prefix, cloze, and information retrieval styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS0P7OUDR5RJ"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer.\n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "prompt_type = 'fixed' ## Use a fixed, or \"manual\", template style.\n",
    "\n",
    "llm = 't5'\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 5\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6br2xnC9Kjc"
   },
   "outputs": [],
   "source": [
    "templ = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} premise: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer was {\"mask\"} .'''\n",
    "#templ = '''{\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} Question: {\"placeholder\":\"text_b\"} ? The answer was {\"mask\"}'''\n",
    "#templ = '''Information: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} Question: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer is {\"mask\"} .'''\n",
    "#templ = '''Question: {\"placeholder\":\"text_b\"} ? Is the answer yes or no? {\"mask\"} . Context: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"}'''\n",
    "#templ = '''Context: {\"placeholder\":\"text_a\", \"post_processing\": lambda x:x if x[-1] == '.' else x+\".\"} . Based on this, of the two options (yes or no), {\"mask\"} is the answer to this question: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x if x[-1] == '?' else x+\"?\"}'''\n",
    "#templ = '''Context: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x if x[-1] == '.' else x+\".\"} Based on the information in this paragraph, {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x if x[-1] == '?' else x+\"?\"} {\"mask\"} .'''\n",
    "#templ = '''premise: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer was {\"mask\"} .'''\n",
    "#templ = '''Information: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} Question: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer is {\"mask\"} .'''\n",
    "#templ = '''The answer to the question \" {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} \" is {\"mask\"} .'''\n",
    "#templ = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} premise: {\"placeholder\":\"text_b\"} The answer was {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-42GIDuN02E"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 2\n",
    "fig2_x = (0, 16, 32, 64, 128, 256, 512)\n",
    "fig2_templ1  = [0.564, 0.489, 0.503, 0.535, 0.638, 0.686, 0.64]\n",
    "fig2_templ1test = 0.649\n",
    "\n",
    "fig2_templ2  = [0.541, 0.504, 0.531, 0.544, 0.546, 0.596, 0.611]\n",
    "fig2_templ2test = 0.611\n",
    "\n",
    "fig2_templ3  = [0.573, 0.5, 0.551, 0.608, 0.531, 0.704, 0.671]\n",
    "fig2_templ3test = 0.670\n",
    "\n",
    "fig2_templ4  = [0.524, 0.499, 0.531, 0.499, 0.529, 0.616, 0.61]\n",
    "fig2_templ4test = 0.583\n",
    "\n",
    "fig2_templ5  = [0.531, 0.502, 0.519, 0.501, 0.526, 0.56, 0.676]\n",
    "fig2_templ5test = 0.667\n",
    "\n",
    "fig2_templ6  = [0.516, 0.485, 0.491, 0.525, 0.539, 0.686, 0.551]\n",
    "fig2_templ6test = 0.571\n",
    "\n",
    "fig2_templ7  = [0.495, 0.492, 0.457, 0.51, 0.508, 0.522, 0.557]\n",
    "fig2_templ7test = 0.539\n",
    "\n",
    "fig2_templ8  = [0.498, 0.487, 0.504, 0.5, 0.51, 0.54, 0.566]\n",
    "fig2_templ8test = 0.533\n",
    "\n",
    "fig2_templ9  = [0.531, 0.506, 0.512, 0.511, 0.518, 0.544, 0.516]\n",
    "fig2_templ9test = 0.497\n",
    "\n",
    "fig2_templ10 = [0.494, 0.496, 0.507, 0.507, 0.505, 0.537, 0.564]\n",
    "fig2_templ10test = 0.534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCHFw6fgUa_T"
   },
   "source": [
    "## Test #3.1\n",
    "Compare fine-tuned soft prompt models with two different starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8FdywTuUhta"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer. \n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "\n",
    "llm = 't5'\n",
    "\n",
    "train_sizes = (128,)\n",
    "n_epochs = 60\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "h4njVMEu9Kic",
    "outputId": "3631aa09-06c7-4ec2-da65-207a0213b5ee"
   },
   "outputs": [],
   "source": [
    "templ1 = '''{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ1,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "id": "K365uNvxVIPh",
    "outputId": "9f9a5ff0-6c5b-45dd-992f-dfbda9891d1a"
   },
   "outputs": [],
   "source": [
    "templ2 = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False} premise: {\"placeholder\":\"text_b\"} The answer was {\"mask\"} .'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ2,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWIhvNVpZSYE"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 3 left panel\n",
    "fig31_x = (0, 1, 10, 20, 30, 40, 50, 60)\n",
    "fig31_templ1 = [0.501, 0.501, 0.511, 0.511, 0.511, 0.511, 0.511, 0.511]\n",
    "fig31_templ2 = [0.558, 0.565, 0.579, 0.584, 0.593, 0.593, 0.585, 0.603]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mly6HPlFYtRv"
   },
   "source": [
    "## Test #3.2\n",
    "\n",
    "Compare four different tuning approaches:\n",
    "\n",
    "1) Soft prompt tuning only\n",
    "\n",
    "2) LLM tuning only\n",
    "\n",
    "3) Soft prompt tuning, then LLM tuning\n",
    "\n",
    "4) Simultaneous soft prompt and LLM tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B260OCIIbR5C"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer.\n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "llm = 't5'\n",
    "\n",
    "templ = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False} premise: {\"placeholder\":\"text_b\"} The answer was {\"mask\"} .'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvm5dv_ybYw3"
   },
   "outputs": [],
   "source": [
    "## 1) Soft prompt tuning only\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V9_h9mVbaNo"
   },
   "outputs": [],
   "source": [
    "## 2) LLM tuning only\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'fixed' ## Use a fixed, or \"manual\", template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MP2xuMfObc6_",
    "outputId": "6eec8b89-7e9d-4491-f23e-1512a0c027a2"
   },
   "outputs": [],
   "source": [
    "## 3) Soft prompt tuning, then LLM tuning\n",
    "\n",
    "train_sizes = (128,)\n",
    "n_epochs = 60\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                        )\n",
    "\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_prompt_to_vary = False,\n",
    "                          starting_mod = prompt_mod\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9daiFYSt9Kg_"
   },
   "outputs": [],
   "source": [
    "## 4) Simultaneous soft prompt and LLM tuning\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIQWXn2mvsO4"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 3 right panel\n",
    "fig32_x = (0, 16, 32, 64, 128, 256, 512)\n",
    "fig32_test1 = [0.558, 0.557, 0.563, 0.585, 0.593, 0.622, 0.645] # Prompt\n",
    "fig32_test2 = [0.551, 0.563, 0.635, 0.664, 0.680, 0.747, 0.755] # LLM\n",
    "fig32_test3 = [0.600, 0.602, 0.555, 0.570, 0.674, 0.741, 0.729] # Prompt > LLM\n",
    "fig32_test4 = [0.558, 0.624, 0.635, 0.683, 0.726, 0.759, 0.761]# Prompt + LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu9AMVHlZYBI"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8kEK52YZab9"
   },
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "or1BqMXdiIfv",
    "outputId": "2dbf02a2-b38e-4f32-b4e6-a77f1df953f0"
   },
   "outputs": [],
   "source": [
    "## Ch 4 tutorial fig 1\n",
    "x = fig1_x\n",
    "y1 = fig1_templ1\n",
    "y2 = fig1_templ2\n",
    "y3 = fig1_templ3\n",
    "y1test = fig1_templ1test\n",
    "y2test = fig1_templ2test\n",
    "y3test = fig1_templ3test\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax11 = fig1.add_subplot(111)\n",
    "\n",
    "ax11.plot(range(len(x)), y1, color='#cc0000', ls='-', lw=2, label='Simplest')\n",
    "ax11.scatter(range(len(x)), y1, color='#cc0000')\n",
    "ax11.plot(range(len(x)), y2, color='#6d9eeb', ls='--', lw=2, label='Simplest + Punct')\n",
    "ax11.scatter(range(len(x)), y2, color='#6d9eeb')\n",
    "ax11.plot(range(len(x)), y3, color='#5CA894', ls=':', lw=2, label='Suitable Template')\n",
    "ax11.scatter(range(len(x)), y3, color='#5CA894')\n",
    "ax11.plot([0,len(x)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax11.text(5.9,0.495,'Coin-flip',ha='right',va='top',color='k',alpha=0.5,fontsize=16)\n",
    "\n",
    "ax11.scatter(6,y1test,color='#cc0000',marker='X',s=200)\n",
    "ax11.scatter(6,y2test,color='#6d9eeb',marker='X',s=200)\n",
    "ax11.scatter(6,y3test,color='#5CA894',marker='X',s=200)\n",
    "\n",
    "ax11.set_xticks(range(len(x)))\n",
    "ax11.set_xticklabels(x,fontsize=16)\n",
    "ax11.set_xlabel('# training examples (per class)',fontsize=20)\n",
    "\n",
    "ax11.set_yticks([0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax11.set_yticklabels(['0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=16)\n",
    "ax11.set_ylabel('Validation Set Accuracy',fontsize=20)\n",
    "\n",
    "ax11.xaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='x', which='major', direction='in')\n",
    "ax11.tick_params(axis='x', which='minor', direction='in')\n",
    "ax11.yaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='y', which='major', direction='in')\n",
    "ax11.tick_params(axis='y', which='minor', direction='in')\n",
    "\n",
    "plt.gca().yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax11.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InH6ygcSZc8p"
   },
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "PD00QmO9fUuR",
    "outputId": "fc10cd64-3f77-4849-f970-3353ba03a160"
   },
   "outputs": [],
   "source": [
    "# Ch4 tutorial fig2\n",
    "\n",
    "ts = fig2_x\n",
    "e1 = fig2_templ1\n",
    "e2 = fig2_templ2\n",
    "e3 = fig2_templ3\n",
    "e4 = fig2_templ4\n",
    "e5 = fig2_templ5\n",
    "e6 = fig2_templ6\n",
    "e7 = fig2_templ7\n",
    "e8 = fig2_templ8\n",
    "e9 = fig2_templ9\n",
    "e10 = fig2_templ10\n",
    "\n",
    "orig_test = fig2_templ1test\n",
    "prefix_test = [fig2_templ2test, fig2_templ3test, fig2_templ4test]\n",
    "cloze_test = [fig2_templ5test, fig2_templ6test]\n",
    "info_test = [fig2_templ7test, fig2_templ8test, fig2_templ9test, fig2_templ10test]\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize=(12,4))\n",
    "ax21 = fig2.add_subplot(131)\n",
    "ax22 = fig2.add_subplot(132)\n",
    "ax23 = fig2.add_subplot(133)\n",
    "\n",
    "bshade_min1 = [min(e2[i],e3[i],e4[i]) for i in range(len(ts))]\n",
    "bshade_max1 = [max(e2[i],e3[i],e4[i]) for i in range(len(ts))]\n",
    "bshade_min2 = [min(e5[i],e6[i]) for i in range(len(ts))]\n",
    "bshade_max2 = [max(e5[i],e6[i]) for i in range(len(ts))]\n",
    "bshade_min3 = [min(e7[i],e8[i],e9[i],e10[i]) for i in range(len(ts))]\n",
    "bshade_max3 = [max(e7[i],e8[i],e9[i],e10[i]) for i in range(len(ts))]\n",
    "\n",
    "ax21.fill_between(range(len(ts)),[i-0.01 for i in e1],[i+0.01 for i in e1],color='k',alpha=0.8, label='Ex. 1')\n",
    "ax21.fill_between(range(len(ts)),bshade_min1,bshade_max1, color='r', alpha=0.6, label='Ex. 2-4')\n",
    "ax21.fill_between(range(len(ts)),bshade_min2,bshade_max2, color='b', alpha=0.1)\n",
    "ax21.fill_between(range(len(ts)),bshade_min3,bshade_max3, color='g', alpha=0.1)\n",
    "\n",
    "ax22.fill_between(range(len(ts)),[i-0.01 for i in e1],[i+0.01 for i in e1],color='k',alpha=0.8, label='Ex. 1')\n",
    "ax22.fill_between(range(len(ts)),bshade_min1,bshade_max1, color='r', alpha=0.1)\n",
    "ax22.fill_between(range(len(ts)),bshade_min2,bshade_max2, color='b', alpha=0.6, label='Ex. 5-6')\n",
    "ax22.fill_between(range(len(ts)),bshade_min3,bshade_max3, color='g', alpha=0.1)\n",
    "\n",
    "ax23.fill_between(range(len(ts)),[i-0.01 for i in e1],[i+0.01 for i in e1],color='k',alpha=0.8, label='Ex. 1')\n",
    "ax23.fill_between(range(len(ts)),bshade_min1,bshade_max1, color='r', alpha=0.1)\n",
    "ax23.fill_between(range(len(ts)),bshade_min2,bshade_max2, color='b', alpha=0.1)\n",
    "ax23.fill_between(range(len(ts)),bshade_min3,bshade_max3, color='g', alpha=0.6, label='Ex. 7-10')\n",
    "\n",
    "ax21.scatter(6,orig_test,color='k',marker='x',s=200)\n",
    "ax21.scatter([6,6,6],prefix_test,color='r',marker='x',s=200,label='Test set eval')\n",
    "ax22.scatter([6,6],cloze_test,color='b',marker='x',s=200,label='Test set eval')\n",
    "ax23.scatter([6,6,6,6],info_test,color='g',marker='x',s=200,label='Test set eval')\n",
    "\n",
    "ax21.plot([0,len(ts)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax21.text(5.9,0.501,'Coin-flip',ha='right',va='bottom',color='k',alpha=0.5)\n",
    "ax22.plot([0,len(ts)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax22.text(5.9,0.501,'Coin-flip',ha='right',va='bottom',color='k',alpha=0.5)\n",
    "ax23.plot([0,len(ts)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax23.text(5.9,0.501,'Coin-flip',ha='right',va='bottom',color='k',alpha=0.5)\n",
    "\n",
    "ax21.set_xticks(range(len(ts)))\n",
    "ax21.set_xticklabels(ts,fontsize=12)\n",
    "ax21.set_xlabel('Number of training examples',fontsize=15)\n",
    "ax21.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax21.set_ylim(0.42, 0.72)\n",
    "ax21.set_yticks([0.45, 0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax21.set_yticklabels(['0.45', '0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=12)\n",
    "ax21.text(0., 0.71,'Prefix\\nPrompts',ha='left',va='top',fontsize=18)\n",
    "ax21.legend(loc=4)\n",
    "\n",
    "ax22.set_xticks(range(len(ts)))\n",
    "ax22.set_xticklabels(ts,fontsize=12)\n",
    "ax22.set_xlabel('Number of training examples',fontsize=15)\n",
    "ax22.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax22.set_ylim(0.42, 0.72)\n",
    "ax22.set_yticks([0.45, 0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax22.set_yticklabels(['0.45', '0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=12)\n",
    "ax22.text(0., 0.71,'Cloze\\nPrompts',ha='left',va='top',fontsize=18)\n",
    "ax22.legend(loc=4)\n",
    "\n",
    "ax23.set_xticks(range(len(ts)))\n",
    "ax23.set_xticklabels(ts,fontsize=12)\n",
    "ax23.set_xlabel('Number of training examples',fontsize=15)\n",
    "ax23.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax23.set_ylim(0.42, 0.72)\n",
    "ax23.set_yticks([0.45, 0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax23.set_yticklabels(['0.45', '0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=12)\n",
    "ax23.text(0., 0.71,'Information\\nRetrieval',ha='left',va='top',fontsize=18)\n",
    "ax23.legend(loc=4)\n",
    "\n",
    "ax21.xaxis.set_ticks_position('both')\n",
    "ax21.tick_params(axis='x', which='major', direction='in')\n",
    "ax21.tick_params(axis='x', which='minor', direction='in')\n",
    "ax21.yaxis.set_ticks_position('both')\n",
    "ax21.tick_params(axis='y', which='major', direction='in')\n",
    "ax21.tick_params(axis='y', which='minor', direction='in')\n",
    "ax21.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax22.xaxis.set_ticks_position('both')\n",
    "ax22.tick_params(axis='x', which='major', direction='in')\n",
    "ax22.tick_params(axis='x', which='minor', direction='in')\n",
    "ax22.yaxis.set_ticks_position('both')\n",
    "ax22.tick_params(axis='y', which='major', direction='in')\n",
    "ax22.tick_params(axis='y', which='minor', direction='in')\n",
    "ax22.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax23.xaxis.set_ticks_position('both')\n",
    "ax23.tick_params(axis='x', which='major', direction='in')\n",
    "ax23.tick_params(axis='x', which='minor', direction='in')\n",
    "ax23.yaxis.set_ticks_position('both')\n",
    "ax23.tick_params(axis='y', which='major', direction='in')\n",
    "ax23.tick_params(axis='y', which='minor', direction='in')\n",
    "ax23.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyvxwjtpZfSP"
   },
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "wx2iyyutWQJp",
    "outputId": "50514808-eca1-4ef8-ddf4-9920da21d2d4"
   },
   "outputs": [],
   "source": [
    "steps_1 = fig31_x\n",
    "c1 = fig31_templ1\n",
    "c2 = fig31_templ2\n",
    "\n",
    "steps_2 = fig32_x\n",
    "c3 = fig32_test1\n",
    "c4 = fig32_test2\n",
    "c5 = fig32_test3\n",
    "c6 = fig32_test4\n",
    "\n",
    "fig3 = plt.figure(figsize=(10,4))\n",
    "ax31 = fig3.add_subplot(121)\n",
    "ax32 = fig3.add_subplot(122)\n",
    "\n",
    "ax31.plot(range(len(steps_1)), c1, label = 'Featureless prompt', color='#cc0000', ls='--', lw=2)\n",
    "ax31.plot(range(len(steps_1)), c2, label = 'Engineered prompt',color='k',ls='-', lw=2)\n",
    "ax31.set_xticks(range(len(steps_1)))\n",
    "ax31.set_xticklabels(steps_1,fontsize=12)\n",
    "ax31.set_xlabel('Number of training epochs',fontsize=15)\n",
    "ax31.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax31.text(0,0.795,'Prompt\\nTuning',ha='left',va='top',fontsize=20)\n",
    "ax31.set_ylim(0.48, 0.81)\n",
    "ax31.set_yticks([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8])\n",
    "ax31.set_yticklabels(['0.50', '0.55', '0.60', '0.65', '0.70', '0.75', '0.80'],fontsize=12)\n",
    "ax31.legend(fontsize=13)\n",
    "\n",
    "ax32.plot(range(len(steps_2)), c3, label = 'Prompt', color='k', ls='-', lw=2)\n",
    "ax32.plot(range(len(steps_2)), c4, label = 'LLM', color='#5CA894', ls='--', lw=2)\n",
    "ax32.plot(range(len(steps_2)), c5, label = 'Prompt -> LLM', color='#cc0000',ls=':', lw=2)\n",
    "ax32.plot(range(len(steps_2)), c6, label = 'Prompt + LLM', color='#6d9eeb', ls='-.', lw=2)\n",
    "ax32.set_xticks(range(len(steps_2)))\n",
    "ax32.set_xticklabels(steps_2,fontsize=12)\n",
    "ax32.set_xlabel('Number of training epochs',fontsize=15)\n",
    "ax32.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax32.text(0,0.795,'Prompt + LLM\\nTuning',ha='left',va='top',fontsize=20)\n",
    "ax32.set_ylim(0.48, 0.81)\n",
    "ax32.set_yticks([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8])\n",
    "ax32.set_yticklabels(['0.50', '0.55', '0.60', '0.65', '0.70', '0.75', '0.80'],fontsize=12)\n",
    "ax32.legend(loc=4,fontsize=10)\n",
    "\n",
    "ax31.xaxis.set_ticks_position('both')\n",
    "ax31.tick_params(axis='x', which='major', direction='in')\n",
    "ax31.tick_params(axis='x', which='minor', direction='in')\n",
    "ax31.yaxis.set_ticks_position('both')\n",
    "ax31.tick_params(axis='y', which='major', direction='in')\n",
    "ax31.tick_params(axis='y', which='minor', direction='in')\n",
    "ax31.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax32.xaxis.set_ticks_position('both')\n",
    "ax32.tick_params(axis='x', which='major', direction='in')\n",
    "ax32.tick_params(axis='x', which='minor', direction='in')\n",
    "ax32.yaxis.set_ticks_position('both')\n",
    "ax32.tick_params(axis='y', which='major', direction='in')\n",
    "ax32.tick_params(axis='y', which='minor', direction='in')\n",
    "ax32.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "fig3.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU80YRiGnJvV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
